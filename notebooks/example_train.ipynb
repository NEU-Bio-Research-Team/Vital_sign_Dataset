{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AKI Prediction - Training Example\n",
        "\n",
        "This notebook demonstrates how to use the modular AKI prediction package to:\n",
        "1. Load and preprocess data\n",
        "2. Train multiple models with hyperparameter tuning\n",
        "3. Evaluate models and save the best one\n",
        "4. Generate SHAP explanations\n",
        "\n",
        "## Simple Usage Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the package\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "\n",
        "# Import all functions from the package\n",
        "from utils import (\n",
        "    setup_plotting, load_vitaldb_data, preprocess_data, prepare_train_test_data\n",
        ")\n",
        "from train import (\n",
        "    get_default_model_configs, hyperparameter_tuning, save_best_model\n",
        ")\n",
        "from evaluate import (\n",
        "    evaluate_models, print_evaluation_summary\n",
        ")\n",
        "from visualization import (\n",
        "    plot_roc_curves, plot_pr_curves, plot_model_comparison, plot_confusion_matrices\n",
        ")\n",
        "from shap_explainer import (\n",
        "    explain_model_with_shap, analyze_logistic_regression_coefficients\n",
        ")\n",
        "\n",
        "# Setup plotting\n",
        "setup_plotting()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Loading VitalDB dataset...\n",
            "âœ… Dataset loaded: 3989 records\n",
            "ðŸ“Š Features available: 75\n",
            "ðŸ”§ Preprocessing data...\n",
            "âœ… Data preprocessing completed\n",
            "ðŸ“Š Final dataset shape: (3989, 43)\n",
            "ðŸŽ¯ Target distribution: 210/3989 positive cases (5.26%)\n",
            "ðŸ”§ Preparing train/test data...\n",
            "ðŸ“Š Training set: (3191, 43)\n",
            "ðŸ“Š Test set: (798, 43)\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "df = load_vitaldb_data()\n",
        "X, y, feature_names = preprocess_data(df)\n",
        "data_dict = prepare_train_test_data(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training with Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Train all models (default)\n",
        "print(\"ðŸŽ¯ Option 1: Training all models...\")\n",
        "models_config_all = get_default_model_configs()\n",
        "# tuned_models = hyperparameter_tuning(models_config_all, data_dict['X_train_dict'], data_dict['y_train'])\n",
        "\n",
        "# Option 2: Train only specific models\n",
        "print(\"\\nðŸŽ¯ Option 2: Training only specific models...\")\n",
        "from train import get_default_model_configs\n",
        "\n",
        "# Get all default configs\n",
        "all_configs = get_default_model_configs()\n",
        "\n",
        "# Select only the models you want to train\n",
        "specific_models = {\n",
        "    'LogisticRegression': all_configs['LogisticRegression'],\n",
        "    'XGBoost': all_configs['XGBoost']\n",
        "    # Uncomment to add more models:\n",
        "    # 'RandomForest': all_configs['RandomForest'],\n",
        "    # 'SVM': all_configs['SVM']\n",
        "}\n",
        "\n",
        "print(f\"Selected models: {list(specific_models.keys())}\")\n",
        "\n",
        "# Train only the selected models\n",
        "tuned_models = hyperparameter_tuning(\n",
        "    specific_models, \n",
        "    data_dict['X_train_dict'], \n",
        "    data_dict['y_train']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Custom Model Configuration (Alternative)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 3: Create custom model configurations with different parameters\n",
        "print(\"ðŸŽ¯ Option 3: Custom model configurations...\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create custom configurations with simplified parameter grids for faster training\n",
        "custom_models = {\n",
        "    'LogisticRegression_Fast': {\n",
        "        'model': LogisticRegression(random_state=0),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],  # Reduced parameter grid\n",
        "            'solver': ['lbfgs'],  # Only one solver\n",
        "            'class_weight': [None, 'balanced']\n",
        "        },\n",
        "        'data_type': 'scaled'\n",
        "    },\n",
        "    'XGBoost_Fast': {\n",
        "        'model': XGBClassifier(random_state=0, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100],  # Reduced parameter grid\n",
        "            'max_depth': [3, 6],\n",
        "            'learning_rate': [0.1, 0.2],\n",
        "            'scale_pos_weight': [1, 18]\n",
        "        },\n",
        "        'data_type': 'imputed'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"Custom models: {list(custom_models.keys())}\")\n",
        "\n",
        "# Uncomment to train custom models instead:\n",
        "# tuned_models = hyperparameter_tuning(\n",
        "#     custom_models, \n",
        "#     data_dict['X_train_dict'], \n",
        "#     data_dict['y_train']\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model data mapping for evaluation (auto-generate based on trained models)\n",
        "model_data_mapping = {}\n",
        "\n",
        "# Auto-generate mapping based on the models that were actually trained\n",
        "for model_name in tuned_models.keys():\n",
        "    if 'Logistic' in model_name or 'SVM' in model_name:\n",
        "        model_data_mapping[model_name] = 'scaled'\n",
        "    else:\n",
        "        model_data_mapping[model_name] = 'imputed'\n",
        "\n",
        "print(f\"Model data mapping: {model_data_mapping}\")\n",
        "\n",
        "# Evaluate all models\n",
        "results_df = evaluate_models(\n",
        "    tuned_models, \n",
        "    data_dict['X_test_dict'], \n",
        "    data_dict['y_test'], \n",
        "    model_data_mapping\n",
        ")\n",
        "\n",
        "# Print summary\n",
        "print_evaluation_summary(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and save the best model\n",
        "best_model_name, best_model = save_best_model(\n",
        "    tuned_models,\n",
        "    data_dict['X_test_dict'],\n",
        "    data_dict['y_test'],\n",
        "    model_data_mapping\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. SHAP Explanations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate SHAP explanations for the best model\n",
        "if 'LogisticRegression' in tuned_models:\n",
        "    lr_model = tuned_models['LogisticRegression']\n",
        "    # Analyze coefficients first\n",
        "    analyze_logistic_regression_coefficients(lr_model, feature_names)\n",
        "    # Generate SHAP explanation\n",
        "    explain_model_with_shap(\n",
        "        lr_model, \n",
        "        data_dict['X_test_dict']['scaled'], \n",
        "        feature_names, \n",
        "        'LogisticRegression', \n",
        "        max_display=15\n",
        "    )\n",
        "\n",
        "# Generate SHAP explanation for XGBoost\n",
        "if 'XGBoost' in tuned_models:\n",
        "    xgb_model = tuned_models['XGBoost']\n",
        "    explain_model_with_shap(\n",
        "        xgb_model, \n",
        "        data_dict['X_test_dict']['imputed'], \n",
        "        feature_names, \n",
        "        'XGBoost', \n",
        "        max_display=15\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ehr-datasets",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
