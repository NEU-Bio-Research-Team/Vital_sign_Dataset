{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5348db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(signals=('ART_MBP', 'CVP', 'NEPI_RATE', 'PLETH_HR', 'PLETH_SPO2'), required_signals=('ART_MBP', 'PLETH_HR', 'PLETH_SPO2'), include_optional_signals=True, fs_hz=1.0, max_len_sec=14400, min_len_sec=600, min_obs_points_per_channel=60, cutoff_mode='early_intraop', t_cut_sec=3600, preop_window_sec=3600, baseline_window_sec=2592000, postop_window_sec=604800, n_splits=5, random_state=42, max_cases_to_cache=None, n_threads=4, api_base='https://api.vitaldb.net', artifacts_dir='artifacts/demo_5signals', cache_dir='artifacts/demo_5signals/cache_npz', tables_dir='artifacts/demo_5signals/tables', scalers_dir='artifacts/demo_5signals/scalers', plots_dir='artifacts/demo_5signals/plots')\n",
      "Saved config: artifacts\\demo_5signals\\config.json\n",
      "GRID_T: start=0.0 sec, end=3600.0 sec, T=3601\n",
      "Ready: imports/config/helpers\n"
     ]
    }
   ],
   "source": [
    "# Step 0 — Imports + Config + Helpers (run first)\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = lambda x, **kwargs: x  # type: ignore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# Config (edit here only)\n",
    "# -------------------------\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    # Signals\n",
    "    signals: Tuple[str, ...] = (\"ART_MBP\", \"CVP\", \"NEPI_RATE\", \"PLETH_HR\", \"PLETH_SPO2\")\n",
    "    required_signals: Tuple[str, ...] = (\"ART_MBP\", \"PLETH_HR\", \"PLETH_SPO2\")  # must-have to keep many cases\n",
    "    include_optional_signals: bool = True  # set False to speed up (skip CVP/NEPI even if available)\n",
    "\n",
    "    # Resampling\n",
    "    fs_hz: float = 1.0\n",
    "    max_len_sec: int = 4 * 3600  # upper cap; grid will auto-trim to cutoff/preop window\n",
    "    min_len_sec: int = 10 * 60   # 10 minutes\n",
    "    min_obs_points_per_channel: int = 60  # observed points per REQUIRED channel\n",
    "\n",
    "    # Feature time window / cutoff (anti-leakage)\n",
    "    cutoff_mode: str = \"early_intraop\"  # one of: \"early_intraop\", \"preop\"\n",
    "    t_cut_sec: int = 60 * 60  # 60 minutes for early_intraop\n",
    "    preop_window_sec: int = 60 * 60  # only used if cutoff_mode=\"preop\"\n",
    "\n",
    "    # AKI label windows (seconds)\n",
    "    baseline_window_sec: int = 30 * 24 * 3600\n",
    "    postop_window_sec: int = 7 * 24 * 3600\n",
    "\n",
    "    # CV / splits\n",
    "    n_splits: int = 5\n",
    "    random_state: int = 42\n",
    "\n",
    "    # Performance\n",
    "    max_cases_to_cache: Optional[int] = None  # set e.g. 500 for a faster demo; None = all\n",
    "    n_threads: int = 4  # parallelize per-case track loads\n",
    "\n",
    "    # IO\n",
    "    api_base: str = \"https://api.vitaldb.net\"\n",
    "    artifacts_dir: str = \"artifacts/demo_5signals\"\n",
    "    cache_dir: str = \"artifacts/demo_5signals/cache_npz\"\n",
    "    tables_dir: str = \"artifacts/demo_5signals/tables\"\n",
    "    scalers_dir: str = \"artifacts/demo_5signals/scalers\"\n",
    "    plots_dir: str = \"artifacts/demo_5signals/plots\"\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "ARTIFACTS_DIR = Path(CFG.artifacts_dir)\n",
    "CACHE_DIR = Path(CFG.cache_dir)\n",
    "TABLES_DIR = Path(CFG.tables_dir)\n",
    "SCALERS_DIR = Path(CFG.scalers_dir)\n",
    "PLOTS_DIR = Path(CFG.plots_dir)\n",
    "for d in [ARTIFACTS_DIR, CACHE_DIR, TABLES_DIR, SCALERS_DIR, PLOTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config alongside artifacts for audit/repro\n",
    "config_path = ARTIFACTS_DIR / \"config.json\"\n",
    "config_path.write_text(json.dumps(asdict(CFG), indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved config: {config_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Utility helpers\n",
    "# -------------------------\n",
    "def _read_or_fetch_csv(name: str, url: str, *, force: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Reads a cached CSV from TABLES_DIR or fetches it from VitalDB API.\"\"\"\n",
    "    path = TABLES_DIR / name\n",
    "    if path.exists() and not force:\n",
    "        return pd.read_csv(path)\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "def _save_json(path: Path, obj) -> None:\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _safe_lower_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.lower()\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame, cols: Iterable[str], df_name: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"{df_name} missing columns: {missing}. Available: {list(df.columns)[:50]}\")\n",
    "\n",
    "def _infer_time_value_cols(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    \"\"\"Infer time/value columns from a track dataframe fetched from the API.\"\"\"\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "    time_candidates = [\"t\", \"time\", \"dt\", \"sec\", \"seconds\"]\n",
    "    value_candidates = [\"v\", \"value\", \"val\", \"y\"]\n",
    "    t_col = next((df.columns[i] for i, c in enumerate(cols) if c in time_candidates), None)\n",
    "    v_col = next((df.columns[i] for i, c in enumerate(cols) if c in value_candidates), None)\n",
    "    if t_col is None or v_col is None:\n",
    "        # fallback: 1st two columns\n",
    "        if df.shape[1] >= 2:\n",
    "            return df.columns[0], df.columns[1]\n",
    "        raise ValueError(f\"Cannot infer time/value cols from columns: {list(df.columns)}\")\n",
    "    return t_col, v_col\n",
    "\n",
    "def _effective_grid_start_end() -> Tuple[float, float]:\n",
    "    \"\"\"Return (start_sec, end_sec) for GRID_T, trimmed to the feature window.\"\"\"\n",
    "    if CFG.cutoff_mode == \"preop\":\n",
    "        start = -float(CFG.preop_window_sec)\n",
    "        end = 0.0\n",
    "        return start, end\n",
    "    # early_intraop: only need up to t_cut_sec\n",
    "    start = 0.0\n",
    "    end = float(min(CFG.max_len_sec, CFG.t_cut_sec))\n",
    "    return start, end\n",
    "\n",
    "def _grid_seconds() -> np.ndarray:\n",
    "    dt = 1.0 / float(CFG.fs_hz)\n",
    "    start, end = _effective_grid_start_end()\n",
    "    # end is exclusive in arange; include endpoint-ish by adding dt\n",
    "    return np.arange(start, end + 1e-6, dt, dtype=np.float32)\n",
    "\n",
    "GRID_T = _grid_seconds()\n",
    "print(f\"GRID_T: start={GRID_T[0]} sec, end={GRID_T[-1]} sec, T={len(GRID_T)}\")\n",
    "\n",
    "# Ordered regex fallback patterns per signal (edit if needed)\n",
    "PATTERNS: Dict[str, List[str]] = {\n",
    "    \"ART_MBP\": [r\"(^|/)ART_MBP$\", r\"ART_MBP\", r\"\\bMBP\\b\"],\n",
    "    \"CVP\": [r\"(^|/)CVP$\", r\"\\bCVP\\b\"],\n",
    "    \"NEPI_RATE\": [r\"(^|/)NEPI_RATE$\", r\"NEPI_RATE\", r\"NOREPI\", r\"NOREP\"],\n",
    "    \"PLETH_HR\": [r\"(^|/)PLETH_HR$\", r\"PLETH_HR\", r\"\\bHR\\b\"],\n",
    "    \"PLETH_SPO2\": [r\"(^|/)PLETH_SPO2$\", r\"PLETH_SPO2\", r\"\\bSPO2\\b\"],\n",
    "}\n",
    "\n",
    "# Per-signal transforms\n",
    "def transform_signal(name: str, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Transform observed signal values in-place (clip/log). Input x is 1D float32.\"\"\"\n",
    "    if name == \"ART_MBP\":\n",
    "        return np.clip(x, 0.0, 200.0)\n",
    "    if name == \"PLETH_HR\":\n",
    "        return np.clip(x, 0.0, 250.0)\n",
    "    if name == \"PLETH_SPO2\":\n",
    "        return np.clip(x, 0.0, 100.0)\n",
    "    if name == \"CVP\":\n",
    "        return np.clip(x, -5.0, 30.0)\n",
    "    if name == \"NEPI_RATE\":\n",
    "        x = np.maximum(x, 0.0)\n",
    "        return np.log1p(x)\n",
    "    return x\n",
    "\n",
    "print(\"Ready: imports/config/helpers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49984bd",
   "metadata": {},
   "source": [
    "### Step 0 — Minimal reproducibility (just enough)\n",
    "- Fix seed, log package versions.\n",
    "- Save artifacts (always): `labels.csv`, `manifest_5signals.csv`, `df_usable.csv`, `folds.json`.\n",
    "\n",
    "### Step 1 — Build AKI label + provenance (**NO leakage**)\n",
    "- Build **baseline Cr** from `dt ∈ [-30d, 0]` (pick the closest-to-0 measurement).\n",
    "- Build **postop max Cr** from `dt ∈ [0, +7d]` (take max).\n",
    "- Define `AKI = 1` if `postop_max_cr >= 1.5 * baseline_cr` else `0`.\n",
    "- Save label provenance (mandatory for audit/debug):\n",
    "  - `baseline_cr`, `baseline_dt`, `postop_max_cr`, `postop_dt_of_max` (if derivable),\n",
    "  - `n_preop_labs`, `n_postop_labs`.\n",
    "- **Baseline missingness policy (MUST choose & document):**\n",
    "  - Prefer **exclude** cases without baseline or without postop labs (to keep labels clean).\n",
    "  - (Optional experiment only) keep them with explicit `label_missing_flag`, but do NOT silently impute.\n",
    "\n",
    "### Step 1.5 — Define feature time window (anti-label-leakage rule) (**MUST enforce**) \n",
    "> Because AKI label is determined from labs in `[0, +7d]`, the modeling goal must be explicit.\n",
    "- Choose exactly one mode for signal extraction:\n",
    "  - **Pre-op**: use signals where `t <= 0` only, OR\n",
    "  - **Early intra-op**: use signals where `t <= t_cut` (e.g., 30–60 min).\n",
    "- Enforce cutoff per case during ingestion: any sample after cutoff must be treated as missing (`mask=0`, value=0).\n",
    "- Save `cutoff_mode`, `t_cut`, and `cutoff_time_used` in cached meta.\n",
    "\n",
    "### Step 2 — Manifest (caseid → tid) for **5 tracks** with strict AND constraint\n",
    "- Required signals: `ART_MBP`, `CVP`, `NEPI_RATE`, `PLETH_HR`, `PLETH_SPO2`.\n",
    "- Build `manifest_5signals.csv` using **ordered regex fallback** per signal.\n",
    "- If multiple tids match a signal:\n",
    "  - Prefer selecting tid by **observed density within cutoff window** (quick-load, resample, choose max `mask.sum()`),\n",
    "  - Not only by “longest duration” (can be long-but-empty/noisy).\n",
    "- Strict AND constraint (for this notebook): keep only cases with **all 5 tids present**.\n",
    "- Save manifest for audit (include chosen tid per signal; optionally keep candidate list).\n",
    "\n",
    "### Step 3 — Ingestion → tensor + mask (missingness-aware) + cache\n",
    "- Load each tid as irregular `(time, value)` series.\n",
    "- Resample irregular → uniform grid (e.g., 1Hz) **without extrapolation**:\n",
    "  - Interpolate only within `[t_min, t_max]` of observed samples; outside = `NaN`.\n",
    "- Build **mask channel** per signal: `mask=1` iff the grid point is truly observed/interpolated from real samples; else `0`.\n",
    "- Replace `NaN → 0` in signal values *after* building mask (so missing stays identifiable).\n",
    "\n",
    "**Transforms / outlier handling (distribution-stable):**\n",
    "- `ART_MBP`: physiologic clip (e.g., `[0, 200]`).\n",
    "- `PLETH_HR`: clip (e.g., `[0, 250]`).\n",
    "- `PLETH_SPO2`: clip (e.g., `[0, 100]`).\n",
    "- `CVP`: require sanity checks + robust clipping:\n",
    "  - Run quantile/range checks to detect unit mismatch/artifacts,\n",
    "  - Prefer **train-fold quantile clip** later (avoid hard-coded thresholds when possible).\n",
    "- `NEPI_RATE`: handle zero-inflation + skew:\n",
    "  - Transform value: `log1p(max(x, 0))`,\n",
    "  - Do NOT conflate missing with zero-dose: the **NEPI mask** must remain a first-class input feature.\n",
    "\n",
    "**Quality gates (MUST):**\n",
    "- Enforce `min_len_sec` after cutoff (exclude too-short cases).\n",
    "- Enforce `min_obs_points` **per channel** using `mask.sum()` (exclude near-empty signals).\n",
    "- Compute `valid_len` from cutoff + observed region (used for masked pooling later).\n",
    "\n",
    "**Cache (per case):**\n",
    "- Save `.npz` with:\n",
    "  - `x` shape `(10, T)` = 5 signals + 5 masks,\n",
    "  - `valid_len`, and meta: `t_min/t_max`, obs counts per channel, cutoff info.\n",
    "\n",
    "### Step 4 — Data QA (must-have before modeling)\n",
    "- Attrition table: `labelled → has_5_tids → pass_cutoff_len → pass_min_obs_each_channel → usable`.\n",
    "- Missingness report per channel: `%mask==1`, plus NEPI `%nonzero among observed`.\n",
    "- Sanity distribution checks per channel (quantiles/histograms) to catch unit mismatch/artifacts (esp. CVP/NEPI).\n",
    "- Plot a few random traces: signal + mask overlays (quick visual verification).\n",
    "\n",
    "### Step 5 — Split folds (case-level only)\n",
    "- Perform `k=5` stratified split by `aki` at **case level** (never by window).\n",
    "- If patient-id exists, use `StratifiedGroupKFold` to avoid patient leakage.\n",
    "- Save `folds.json` (list of caseids per fold) for reproducibility.\n",
    "\n",
    "### Step 6 — Per-fold normalization (train-only, mask-aware) (**critical**)\n",
    "- Fit normalization stats on **train-fold only** using only values where `mask==1`.\n",
    "- Apply to train/val; keep missing points at `0` with `mask=0`.\n",
    "- Prefer robust scaling where needed:\n",
    "  - `NEPI_RATE`: `log1p` then **median/IQR** (often better than mean/std),\n",
    "  - `CVP`: consider train-fold quantile clip + (mean/std or robust).\n",
    "- Save per-fold scalers for audit/inference consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90106add",
   "metadata": {},
   "source": [
    "## 1) Load VitalDB tables + build AKI labels (with provenance)\n",
    "This section creates `labels.csv` (case-level AKI labels) from `/labs` using creatinine (`cr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7df1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases: (6388, 74) columns: ['caseid', 'subjectid', 'casestart', 'caseend', 'anestart', 'aneend', 'opstart', 'opend', 'adm', 'dis', 'icu_days', 'death_inhosp']\n",
      "trks : (486449, 3) columns: ['caseid', 'tname', 'tid']\n",
      "labs : (928448, 4) columns: ['caseid', 'dt', 'name', 'result']\n",
      "Saved labels: artifacts\\demo_5signals\\labels.csv\n",
      "Labelled cases: 3242  AKI positive: 127\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>baseline_dt</th>\n",
       "      <th>baseline_cr</th>\n",
       "      <th>postop_dt_of_max</th>\n",
       "      <th>postop_max_cr</th>\n",
       "      <th>n_preop_labs</th>\n",
       "      <th>n_postop_labs</th>\n",
       "      <th>aki</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-2968</td>\n",
       "      <td>1.83</td>\n",
       "      <td>131894</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>-97335</td>\n",
       "      <td>0.84</td>\n",
       "      <td>16337</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>-91787</td>\n",
       "      <td>0.58</td>\n",
       "      <td>166708</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>-55795</td>\n",
       "      <td>0.86</td>\n",
       "      <td>89931</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>-81225</td>\n",
       "      <td>0.91</td>\n",
       "      <td>150205</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseid  baseline_dt  baseline_cr  postop_dt_of_max  postop_max_cr  \\\n",
       "0       5        -2968         1.83            131894           4.43   \n",
       "1       7       -97335         0.84             16337           0.82   \n",
       "2      12       -91787         0.58            166708           0.65   \n",
       "3      13       -55795         0.86             89931           0.78   \n",
       "4      16       -81225         0.91            150205           1.02   \n",
       "\n",
       "   n_preop_labs  n_postop_labs  aki  \n",
       "0             1             18    1  \n",
       "1             1              4    0  \n",
       "2             1             16    0  \n",
       "3             1              2    0  \n",
       "4             1              5    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 — Load /cases, /trks, /labs (cached) + build labels\n",
    "cases_url = f\"{CFG.api_base}/cases\"\n",
    "trks_url = f\"{CFG.api_base}/trks\"\n",
    "labs_url = f\"{CFG.api_base}/labs\"\n",
    "\n",
    "df_cases = _read_or_fetch_csv(\"cases.csv\", cases_url)\n",
    "df_trks = _read_or_fetch_csv(\"trks.csv\", trks_url)\n",
    "df_labs = _read_or_fetch_csv(\"labs.csv\", labs_url)\n",
    "\n",
    "print(\"cases:\", df_cases.shape, \"columns:\", list(df_cases.columns)[:12])\n",
    "print(\"trks :\", df_trks.shape, \"columns:\", list(df_trks.columns)[:12])\n",
    "print(\"labs :\", df_labs.shape, \"columns:\", list(df_labs.columns)[:12])\n",
    "\n",
    "# --- Label builder (AKI from Cr) ---\n",
    "def build_aki_labels_from_labs(\n",
    "    labs: pd.DataFrame,\n",
    "    *,\n",
    "    baseline_window_sec: int,\n",
    "    postop_window_sec: int,\n",
    "    name_col: str = \"name\",\n",
    "    dt_col: str = \"dt\",\n",
    "    result_col: str = \"result\",\n",
    "    caseid_col: str = \"caseid\",\n",
    "    cr_names: Tuple[str, ...] = (\"cr\", \"creatinine\"),\n",
    ") -> pd.DataFrame:\n",
    "    labs = labs.copy()\n",
    "    _ensure_cols(labs, [caseid_col, name_col, dt_col, result_col], \"labs\")\n",
    "    labs[name_col] = _safe_lower_series(labs[name_col])\n",
    "    labs = labs[labs[name_col].isin(cr_names)].copy()\n",
    "    labs[dt_col] = pd.to_numeric(labs[dt_col], errors=\"coerce\")\n",
    "    labs[result_col] = pd.to_numeric(labs[result_col], errors=\"coerce\")\n",
    "    labs = labs.dropna(subset=[caseid_col, dt_col, result_col])\n",
    "    labs[caseid_col] = pd.to_numeric(labs[caseid_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    labs = labs.dropna(subset=[caseid_col])\n",
    "    labs[caseid_col] = labs[caseid_col].astype(int)\n",
    "\n",
    "    # Baseline window: [-baseline_window_sec, 0]\n",
    "    pre = labs[(labs[dt_col] <= 0) & (labs[dt_col] >= -baseline_window_sec)].copy()\n",
    "    pre = pre.sort_values([caseid_col, dt_col])\n",
    "    # Closest-to-0 is max dt within window\n",
    "    idx_base = pre.groupby(caseid_col)[dt_col].idxmax()\n",
    "    baseline = pre.loc[idx_base, [caseid_col, dt_col, result_col]].rename(\n",
    "        columns={dt_col: \"baseline_dt\", result_col: \"baseline_cr\"}\n",
    "    )\n",
    "\n",
    "    # Postop window: [0, postop_window_sec]\n",
    "    post = labs[(labs[dt_col] >= 0) & (labs[dt_col] <= postop_window_sec)].copy()\n",
    "    post = post.sort_values([caseid_col, result_col, dt_col])\n",
    "    # Max Cr; for ties pick latest dt\n",
    "    idx_post = post.groupby(caseid_col)[result_col].idxmax()\n",
    "    postop = post.loc[idx_post, [caseid_col, dt_col, result_col]].rename(\n",
    "        columns={dt_col: \"postop_dt_of_max\", result_col: \"postop_max_cr\"}\n",
    "    )\n",
    "\n",
    "    # Counts for provenance\n",
    "    n_pre = pre.groupby(caseid_col).size().rename(\"n_preop_labs\")\n",
    "    n_post = post.groupby(caseid_col).size().rename(\"n_postop_labs\")\n",
    "\n",
    "    out = baseline.merge(postop, on=caseid_col, how=\"inner\")\n",
    "    out = out.merge(n_pre, on=caseid_col, how=\"left\").merge(n_post, on=caseid_col, how=\"left\")\n",
    "    out[\"aki\"] = (out[\"postop_max_cr\"] >= 1.5 * out[\"baseline_cr\"]).astype(int)\n",
    "    out = out.sort_values(caseid_col).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "df_labels = build_aki_labels_from_labs(\n",
    "    df_labs,\n",
    "    baseline_window_sec=CFG.baseline_window_sec,\n",
    "    postop_window_sec=CFG.postop_window_sec,\n",
    ")\n",
    "\n",
    "labels_path = ARTIFACTS_DIR / \"labels.csv\"\n",
    "df_labels.to_csv(labels_path, index=False)\n",
    "print(f\"Saved labels: {labels_path}\")\n",
    "print(\"Labelled cases:\", len(df_labels), \" AKI positive:\", int(df_labels['aki'].sum()))\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd9f81",
   "metadata": {},
   "source": [
    "## 2) Build manifest (caseid → tid) for 5 tracks\n",
    "This section creates `manifest_5signals.csv` by mapping `tname` → `tid` using ordered regex fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a42a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks restricted to labelled cases: (258130, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09da08d5e94a4450a27147174f405dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building manifest:   0%|          | 0/3242 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved manifest: artifacts\\demo_5signals\\manifest_relaxed.csv\n",
      "Cases with required signals: 2504  / labelled: 3242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>tid_ART_MBP</th>\n",
       "      <th>tid_CVP</th>\n",
       "      <th>tid_NEPI_RATE</th>\n",
       "      <th>tid_PLETH_HR</th>\n",
       "      <th>tid_PLETH_SPO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>30a4c59beb2c29881c969ff720e489aba3465c30</td>\n",
       "      <td>20c3c458496ea1fc637f3d94122e33167ef5218e</td>\n",
       "      <td>None</td>\n",
       "      <td>f3cce12094dc3734fbee47ba2180c990ca51e348</td>\n",
       "      <td>a81bc2962fc0ac48297eb856ff3710dda5c3a276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>68c782020e500fc51e9b47352805e9ffdc088de4</td>\n",
       "      <td>5f67c82cf764ac4972b0154d992ddfae5319d696</td>\n",
       "      <td>None</td>\n",
       "      <td>6b479f97477ba56ddcc72a86ac5978f274694b9a</td>\n",
       "      <td>dff0c94054cae91ab7849559851b5c0b663e6022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>1f13c262719341c545ec38de4098a5b0b9e51144</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>35ad379e06247069984ecb20ec025dee703b922d</td>\n",
       "      <td>e4ae3d53facdbefea02bbf8b757e16f69dd06bc9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0254d97febeb440b551252ac4c5d2ef2740d4172</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>814376860936ea95a151626820bd7f0adbc0e2b2</td>\n",
       "      <td>0780cae61d8e7a775654c3b94f35df25cb5a3df7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>c2f593e2322e78d975ad049c1671537f2974fda7</td>\n",
       "      <td>1ed0dd3eaf16bcbb4f3930bf576b076fc7ff5b60</td>\n",
       "      <td>a5b6a3a6d146104c5a912f96d306b37a055f593f</td>\n",
       "      <td>880b8774d9297abf030b2df878814f413d75451f</td>\n",
       "      <td>f0d3fa0a4dde9d7506127ddb497af383e83c5c12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseid                               tid_ART_MBP  \\\n",
       "0       7  30a4c59beb2c29881c969ff720e489aba3465c30   \n",
       "1      12  68c782020e500fc51e9b47352805e9ffdc088de4   \n",
       "2      13  1f13c262719341c545ec38de4098a5b0b9e51144   \n",
       "3      16  0254d97febeb440b551252ac4c5d2ef2740d4172   \n",
       "4      17  c2f593e2322e78d975ad049c1671537f2974fda7   \n",
       "\n",
       "                                    tid_CVP  \\\n",
       "0  20c3c458496ea1fc637f3d94122e33167ef5218e   \n",
       "1  5f67c82cf764ac4972b0154d992ddfae5319d696   \n",
       "2                                      None   \n",
       "3                                      None   \n",
       "4  1ed0dd3eaf16bcbb4f3930bf576b076fc7ff5b60   \n",
       "\n",
       "                              tid_NEPI_RATE  \\\n",
       "0                                      None   \n",
       "1                                      None   \n",
       "2                                      None   \n",
       "3                                      None   \n",
       "4  a5b6a3a6d146104c5a912f96d306b37a055f593f   \n",
       "\n",
       "                               tid_PLETH_HR  \\\n",
       "0  f3cce12094dc3734fbee47ba2180c990ca51e348   \n",
       "1  6b479f97477ba56ddcc72a86ac5978f274694b9a   \n",
       "2  35ad379e06247069984ecb20ec025dee703b922d   \n",
       "3  814376860936ea95a151626820bd7f0adbc0e2b2   \n",
       "4  880b8774d9297abf030b2df878814f413d75451f   \n",
       "\n",
       "                             tid_PLETH_SPO2  \n",
       "0  a81bc2962fc0ac48297eb856ff3710dda5c3a276  \n",
       "1  dff0c94054cae91ab7849559851b5c0b663e6022  \n",
       "2  e4ae3d53facdbefea02bbf8b757e16f69dd06bc9  \n",
       "3  0780cae61d8e7a775654c3b94f35df25cb5a3df7  \n",
       "4  f0d3fa0a4dde9d7506127ddb497af383e83c5c12  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 — Build manifest_5signals.csv (relaxed: keep many cases)\n",
    "_ensure_cols(df_trks, [\"caseid\", \"tid\", \"tname\"], \"trks\")\n",
    "df_trks = df_trks.copy()\n",
    "df_trks[\"caseid\"] = pd.to_numeric(df_trks[\"caseid\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_trks = df_trks.dropna(subset=[\"caseid\"]).copy()\n",
    "df_trks[\"caseid\"] = df_trks[\"caseid\"].astype(int)\n",
    "df_trks[\"tid\"] = df_trks[\"tid\"].astype(str)\n",
    "df_trks = df_trks[df_trks[\"tid\"].str.len() > 0].copy()\n",
    "df_trks[\"tname\"] = df_trks[\"tname\"].astype(str)\n",
    "\n",
    "# Guardrails: the manifest will be empty if there are no labelled cases\n",
    "if \"df_labels\" not in globals() or df_labels is None or len(df_labels) == 0:\n",
    "    raise ValueError(\n",
    "        \"df_labels is empty, so there are no labelled caseids to build a manifest from. \"\n",
    "        \"This usually means the AKI label builder found no usable baseline/postop creatinine labs. \"\n",
    "        \"Check Step 1 outputs (how many Cr labs were found, and that labs['name'] contains 'cr'/'creatinine').\"\n",
    "    )\n",
    "\n",
    "label_caseids = set(df_labels[\"caseid\"].astype(int).tolist())\n",
    "trks_lab = df_trks[df_trks[\"caseid\"].isin(label_caseids)].copy()\n",
    "print(\"Tracks restricted to labelled cases:\", trks_lab.shape)\n",
    "if len(trks_lab) == 0:\n",
    "    print(\"No tracks matched labelled caseids.\")\n",
    "    print(\"n_labelled_caseids:\", len(label_caseids), \"  n_total_tracks:\", len(df_trks))\n",
    "    raise ValueError(\n",
    "        \"No tracks available after restricting to labelled cases. \"\n",
    "        \"Either df_labels caseids do not exist in df_trks, or df_labels is built from a different dataset/time.\"\n",
    "    )\n",
    "\n",
    "import re\n",
    "def pick_tid_for_signal(group: pd.DataFrame, signal: str) -> Optional[str]:\n",
    "    \"\"\"Pick a single tid for a signal using ordered regex fallback on tname.\"\"\"\n",
    "    pats = PATTERNS.get(signal, [signal])\n",
    "    tname = group[\"tname\"].astype(str)\n",
    "    for pat in pats:\n",
    "        m = tname.str.contains(pat, regex=True, na=False)\n",
    "        if m.any():\n",
    "            return str(group.loc[m, \"tid\"].iloc[0])\n",
    "    return None\n",
    "\n",
    "# Build manifest row-by-row (case-level)\n",
    "manifest_rows = []\n",
    "for caseid, g in tqdm(trks_lab.groupby(\"caseid\"), desc=\"Building manifest\", unit=\"case\"):\n",
    "    row = {\"caseid\": int(caseid)}\n",
    "    for sig in CFG.signals:\n",
    "        row[f\"tid_{sig}\"] = pick_tid_for_signal(g, sig)\n",
    "    manifest_rows.append(row)\n",
    "\n",
    "tid_cols = [f\"tid_{s}\" for s in CFG.signals]\n",
    "expected_cols = [\"caseid\", *tid_cols]\n",
    "df_manifest = pd.DataFrame(manifest_rows, columns=expected_cols)\n",
    "\n",
    "# Relaxed cohort: keep cases that have ALL required_signals; allow missing optional signals\n",
    "required_tid_cols = [f\"tid_{s}\" for s in CFG.required_signals]\n",
    "df_manifest_relaxed = df_manifest.dropna(subset=required_tid_cols).copy()\n",
    "df_manifest_relaxed = df_manifest_relaxed.sort_values(\"caseid\").reset_index(drop=True)\n",
    "\n",
    "manifest_path = ARTIFACTS_DIR / \"manifest_relaxed.csv\"\n",
    "df_manifest_relaxed.to_csv(manifest_path, index=False)\n",
    "print(f\"Saved manifest: {manifest_path}\")\n",
    "print(\"Cases with required signals:\", len(df_manifest_relaxed), \" / labelled:\", len(df_labels))\n",
    "\n",
    "# Keep df_manifest_all name for downstream cells\n",
    "df_manifest_all = df_manifest_relaxed\n",
    "df_manifest_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7087f40",
   "metadata": {},
   "source": [
    "## 3) Ingest 5 tracks → resample → (5 signals + 5 masks) tensor → cache npz\n",
    "This section writes per-case `.npz` tensors and produces `df_usable.csv` with quality gates enforced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467dc2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff mode: early_intraop cutoff_sec: 3600.0\n",
      "Caching cases: 2504 (of 2504 )\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d399c8a04642d59b495f7ea6415213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Caching cases:   0%|          | 0/2504 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved usable: artifacts\\demo_5signals\\df_usable.csv (2413 cases)\n",
      "Saved failed: artifacts\\demo_5signals\\df_failed.csv (91 cases)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>cache_path</th>\n",
       "      <th>status</th>\n",
       "      <th>obs_ART_MBP</th>\n",
       "      <th>obs_CVP</th>\n",
       "      <th>obs_NEPI_RATE</th>\n",
       "      <th>obs_PLETH_HR</th>\n",
       "      <th>obs_PLETH_SPO2</th>\n",
       "      <th>valid_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>artifacts\\demo_5signals\\cache_npz\\case_7.npz</td>\n",
       "      <td>cached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>artifacts\\demo_5signals\\cache_npz\\case_12.npz</td>\n",
       "      <td>cached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>artifacts\\demo_5signals\\cache_npz\\case_13.npz</td>\n",
       "      <td>cached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>artifacts\\demo_5signals\\cache_npz\\case_16.npz</td>\n",
       "      <td>cached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>artifacts\\demo_5signals\\cache_npz\\case_17.npz</td>\n",
       "      <td>cached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseid                                     cache_path  status  obs_ART_MBP  \\\n",
       "0       7   artifacts\\demo_5signals\\cache_npz\\case_7.npz  cached          NaN   \n",
       "1      12  artifacts\\demo_5signals\\cache_npz\\case_12.npz  cached          NaN   \n",
       "2      13  artifacts\\demo_5signals\\cache_npz\\case_13.npz  cached          NaN   \n",
       "3      16  artifacts\\demo_5signals\\cache_npz\\case_16.npz  cached          NaN   \n",
       "4      17  artifacts\\demo_5signals\\cache_npz\\case_17.npz  cached          NaN   \n",
       "\n",
       "   obs_CVP  obs_NEPI_RATE  obs_PLETH_HR  obs_PLETH_SPO2  valid_len  \n",
       "0      NaN            NaN           NaN             NaN        NaN  \n",
       "1      NaN            NaN           NaN             NaN        NaN  \n",
       "2      NaN            NaN           NaN             NaN        NaN  \n",
       "3      NaN            NaN           NaN             NaN        NaN  \n",
       "4      NaN            NaN           NaN             NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3 — Ingestion helpers + cache writer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# -------------------------\n",
    "# Robust HTTP (timeout + retry)\n",
    "_thread_local = threading.local()\n",
    "\n",
    "def _get_session() -> requests.Session:\n",
    "    s = getattr(_thread_local, \"session\", None)\n",
    "    if s is None:\n",
    "        s = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=5,\n",
    "            connect=5,\n",
    "            read=5,\n",
    "            backoff_factor=0.8,\n",
    "            status_forcelist=(429, 500, 502, 503, 504),\n",
    "            allowed_methods=(\"GET\",),\n",
    "            raise_on_status=False,\n",
    "            respect_retry_after_header=True,\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=16, pool_maxsize=16)\n",
    "        s.mount(\"http://\", adapter)\n",
    "        s.mount(\"https://\", adapter)\n",
    "        _thread_local.session = s\n",
    "    return s\n",
    "\n",
    "def fetch_track_df(tid: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch a track by tid from VitalDB API with timeouts + retries.\"\"\"\n",
    "    url = f\"{CFG.api_base}/{str(tid)}\"\n",
    "    sess = _get_session()\n",
    "    # Use explicit timeouts to prevent hanging forever\n",
    "    r = sess.get(url, timeout=(5, 30))\n",
    "    # If still rate-limited after retries, sleep briefly and try once more\n",
    "    if r.status_code == 429:\n",
    "        retry_after = r.headers.get(\"Retry-After\")\n",
    "        try:\n",
    "            wait = float(retry_after) if retry_after is not None else 5.0\n",
    "        except Exception:\n",
    "            wait = 5.0\n",
    "        time.sleep(min(max(wait, 1.0), 30.0))\n",
    "        r = sess.get(url, timeout=(5, 30))\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code} fetching tid={tid}\")\n",
    "    # Pandas can read from in-memory text buffer\n",
    "    return pd.read_csv(io.StringIO(r.text))\n",
    "\n",
    "def load_track_df_cached(tid: str, *, force: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Fetch track CSV once and cache under TABLES_DIR/tracks/{tid}.csv (atomic write).\"\"\"\n",
    "    track_dir = TABLES_DIR / \"tracks\"\n",
    "    track_dir.mkdir(parents=True, exist_ok=True)\n",
    "    safe_tid = str(tid)\n",
    "    path = track_dir / f\"{safe_tid}.csv\"\n",
    "    if path.exists() and not force:\n",
    "        try:\n",
    "            # Guard against truncated/empty cache files (can happen if a previous run was interrupted)\n",
    "            if path.stat().st_size < 64:\n",
    "                raise ValueError(\"cache file too small\")\n",
    "            return pd.read_csv(path)\n",
    "        except Exception:\n",
    "            # fall through to refetch\n",
    "            pass\n",
    "    df = fetch_track_df(safe_tid)\n",
    "    tmp_path = track_dir / f\"{safe_tid}.csv.tmp\"\n",
    "    df.to_csv(tmp_path, index=False)\n",
    "    os.replace(str(tmp_path), str(path))\n",
    "    return df\n",
    "\n",
    "def resample_to_grid(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    grid_t: np.ndarray,\n",
    "    *,\n",
    "    cutoff_sec: float,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Resample irregular (times, values) onto grid_t using interpolation, without extrapolation.\n",
    "    Returns (signal, mask) where mask==1 indicates interpolated/observed inside data support and before cutoff.\n",
    "    \"\"\"\n",
    "    times = np.asarray(times, dtype=np.float64)\n",
    "    values = np.asarray(values, dtype=np.float64)\n",
    "    ok = np.isfinite(times) & np.isfinite(values)\n",
    "    times = times[ok]\n",
    "    values = values[ok]\n",
    "    if times.size == 0:\n",
    "        sig = np.zeros_like(grid_t, dtype=np.float32)\n",
    "        mask = np.zeros_like(grid_t, dtype=np.float32)\n",
    "        return sig, mask\n",
    "\n",
    "    order = np.argsort(times)\n",
    "    times = times[order]\n",
    "    values = values[order]\n",
    "\n",
    "    # Remove duplicate times (keep last)\n",
    "    times_rev = times[::-1]\n",
    "    values_rev = values[::-1]\n",
    "    _, uniq_idx_rev = np.unique(times_rev, return_index=True)\n",
    "    keep_rev = np.sort(uniq_idx_rev)\n",
    "    times = times_rev[keep_rev][::-1]\n",
    "    values = values_rev[keep_rev][::-1]\n",
    "\n",
    "    t_min = float(np.min(times))\n",
    "    t_max = float(np.max(times))\n",
    "\n",
    "    inside = (grid_t >= t_min) & (grid_t <= t_max) & (grid_t <= cutoff_sec)\n",
    "    sig = np.full_like(grid_t, np.nan, dtype=np.float32)\n",
    "    mask = np.zeros_like(grid_t, dtype=np.float32)\n",
    "    if inside.any():\n",
    "        sig[inside] = np.interp(grid_t[inside].astype(np.float64), times, values).astype(np.float32)\n",
    "        mask[inside] = 1.0\n",
    "    sig = np.nan_to_num(sig, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return sig, mask\n",
    "\n",
    "def get_cutoff_sec() -> float:\n",
    "    if CFG.cutoff_mode == \"preop\":\n",
    "        return 0.0\n",
    "    return float(min(CFG.t_cut_sec, CFG.max_len_sec))\n",
    "\n",
    "CUTOFF_SEC = get_cutoff_sec()\n",
    "print(\"Cutoff mode:\", CFG.cutoff_mode, \"cutoff_sec:\", CUTOFF_SEC)\n",
    "\n",
    "def _tid_or_none(v) -> Optional[str]:\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, float) and np.isnan(v):\n",
    "        return None\n",
    "    s = str(v).strip()\n",
    "    if s == \"\" or s.lower() in (\"nan\", \"none\", \"null\"):\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "def _process_one_signal(sig_name: str, tid: str) -> Tuple[str, np.ndarray, np.ndarray, int]:\n",
    "    df_track = load_track_df_cached(tid)\n",
    "    t_col, v_col = _infer_time_value_cols(df_track)\n",
    "    t = pd.to_numeric(df_track[t_col], errors=\"coerce\").to_numpy(dtype=np.float64)\n",
    "    v = pd.to_numeric(df_track[v_col], errors=\"coerce\").to_numpy(dtype=np.float64)\n",
    "    sig, mask = resample_to_grid(t, v, GRID_T, cutoff_sec=CUTOFF_SEC)\n",
    "    sig = transform_signal(sig_name, sig.astype(np.float32))\n",
    "    sig = sig * mask\n",
    "    return sig_name, sig.astype(np.float32), mask.astype(np.float32), int(mask.sum())\n",
    "\n",
    "def build_case_tensor_from_manifest_row(row: pd.Series, *, executor: ThreadPoolExecutor) -> Tuple[np.ndarray, int, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Returns x (10,T), valid_len, obs_counts per signal index.\n",
    "    Missing optional signals are allowed: their (signal,mask) will be all zeros.\n",
    "    \"\"\"\n",
    "    T = len(GRID_T)\n",
    "    sig_names = list(CFG.signals)\n",
    "    n_sig = len(sig_names)\n",
    "    sig_mat = np.zeros((n_sig, T), dtype=np.float32)\n",
    "    mask_mat = np.zeros((n_sig, T), dtype=np.float32)\n",
    "    obs_counts: Dict[str, int] = {name: 0 for name in sig_names}\n",
    "\n",
    "    futures = {}\n",
    "    for name in sig_names:\n",
    "        is_required = name in CFG.required_signals\n",
    "        is_optional = not is_required\n",
    "        if is_optional and not CFG.include_optional_signals:\n",
    "            continue\n",
    "        tid = _tid_or_none(row.get(f\"tid_{name}\"))\n",
    "        if tid is None:\n",
    "            continue\n",
    "        futures[executor.submit(_process_one_signal, name, tid)] = name\n",
    "\n",
    "    for fut in as_completed(futures):\n",
    "        name = futures[fut]\n",
    "        try:\n",
    "            sig_name, sig, mask, n_obs = fut.result()\n",
    "        except Exception:\n",
    "            # Treat a failed download/parse as missing for this signal (do NOT kill the whole run)\n",
    "            obs_counts[name] = 0\n",
    "            continue\n",
    "        i = sig_names.index(sig_name)\n",
    "        sig_mat[i] = sig\n",
    "        mask_mat[i] = mask\n",
    "        obs_counts[sig_name] = int(n_obs)\n",
    "\n",
    "    any_mask = (mask_mat.sum(axis=0) > 0)\n",
    "    valid_len = int(np.max(np.where(any_mask)[0]) + 1) if any_mask.any() else 0\n",
    "    x = np.concatenate([sig_mat, mask_mat], axis=0).astype(np.float32)\n",
    "    return x, valid_len, obs_counts\n",
    "\n",
    "def quality_gates(obs_counts: Dict[str, int], valid_len: int) -> Tuple[bool, str]:\n",
    "    for sig_name in CFG.required_signals:\n",
    "        if obs_counts.get(sig_name, 0) < CFG.min_obs_points_per_channel:\n",
    "            return False, f\"min_obs_fail_required:{sig_name}\"\n",
    "    if valid_len < int(CFG.min_len_sec * CFG.fs_hz):\n",
    "        return False, \"min_len_fail\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "# Build cache for all manifest cases (optionally limit for faster runs)\n",
    "df_manifest_run = df_manifest_all\n",
    "if CFG.max_cases_to_cache is not None:\n",
    "    df_manifest_run = df_manifest_run.head(int(CFG.max_cases_to_cache)).copy()\n",
    "print(\"Caching cases:\", len(df_manifest_run), \"(of\", len(df_manifest_all), \")\")\n",
    "\n",
    "usable_rows = []\n",
    "failed_rows = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=int(CFG.n_threads)) as ex:\n",
    "    for _, row in tqdm(df_manifest_run.iterrows(), total=len(df_manifest_run), desc=\"Caching cases\", unit=\"case\"):\n",
    "        caseid = int(row[\"caseid\"])\n",
    "        out_path = CACHE_DIR / f\"case_{caseid}.npz\"\n",
    "        if out_path.exists():\n",
    "            usable_rows.append({\"caseid\": caseid, \"cache_path\": str(out_path), \"status\": \"cached\"})\n",
    "            continue\n",
    "        try:\n",
    "            x, valid_len, obs_counts = build_case_tensor_from_manifest_row(row, executor=ex)\n",
    "            ok, reason = quality_gates(obs_counts, valid_len)\n",
    "            if not ok:\n",
    "                failed_rows.append({\"caseid\": caseid, \"reason\": reason, **{f\"obs_{k}\": v for k, v in obs_counts.items()}, \"valid_len\": valid_len})\n",
    "                continue\n",
    "            np.savez_compressed(\n",
    "                out_path,\n",
    "                x=x,\n",
    "                valid_len=np.int32(valid_len),\n",
    "                grid_t=GRID_T,\n",
    "                cutoff_mode=np.array([CFG.cutoff_mode]),\n",
    "                cutoff_sec=np.float32(CUTOFF_SEC),\n",
    "                obs_counts=np.array([obs_counts], dtype=object),\n",
    "            )\n",
    "            usable_rows.append({\"caseid\": caseid, \"cache_path\": str(out_path), \"status\": \"new\", **{f\"obs_{k}\": v for k, v in obs_counts.items()}, \"valid_len\": valid_len})\n",
    "        except Exception as e:\n",
    "            failed_rows.append({\"caseid\": caseid, \"reason\": f\"exception:{type(e).__name__}\", \"detail\": str(e)[:200]})\n",
    "\n",
    "df_usable = pd.DataFrame(usable_rows)\n",
    "df_failed = pd.DataFrame(failed_rows)\n",
    "usable_path = ARTIFACTS_DIR / \"df_usable.csv\"\n",
    "failed_path = ARTIFACTS_DIR / \"df_failed.csv\"\n",
    "df_usable.to_csv(usable_path, index=False)\n",
    "df_failed.to_csv(failed_path, index=False)\n",
    "print(f\"Saved usable: {usable_path} ({len(df_usable)} cases)\")\n",
    "print(f\"Saved failed: {failed_path} ({len(df_failed)} cases)\")\n",
    "df_usable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a796",
   "metadata": {},
   "source": [
    "## 4) Data QA (attrition + missingness + sanity quantiles)\n",
    "Run this after caching to verify distribution and detect unit/artifact issues (especially CVP/NEPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8981b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Attrition ---\n",
      "{'labelled': 3242, 'has_5_tids': 2504, 'usable_cached': 2413}\n",
      "\n",
      "--- Label distribution (usable only) ---\n",
      "aki\n",
      "0    2312\n",
      "1     101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Missingness report from cache metadata ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>obs_ART_MBP</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obs_CVP</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obs_NEPI_RATE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>3596.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obs_PLETH_HR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obs_PLETH_SPO2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "      <td>3594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid_len</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>3601.0</td>\n",
       "      <td>3601.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count    mean  std     min     25%     50%     75%     max\n",
       "obs_ART_MBP       1.0  3594.0  NaN  3594.0  3594.0  3594.0  3594.0  3594.0\n",
       "obs_CVP           1.0     0.0  NaN     0.0     0.0     0.0     0.0     0.0\n",
       "obs_NEPI_RATE     1.0  3596.0  NaN  3596.0  3596.0  3596.0  3596.0  3596.0\n",
       "obs_PLETH_HR      1.0  3594.0  NaN  3594.0  3594.0  3594.0  3594.0  3594.0\n",
       "obs_PLETH_SPO2    1.0  3594.0  NaN  3594.0  3594.0  3594.0  3594.0  3594.0\n",
       "valid_len         1.0  3601.0  NaN  3601.0  3601.0  3601.0  3601.0  3601.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample quantiles by channel (observed values only; from a small sample) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47b26aeb2aa465496f1442cf7042bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantiles:   0%|          | 0/50 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q01</th>\n",
       "      <th>q50</th>\n",
       "      <th>q99</th>\n",
       "      <th>n_obs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ART_MBP</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>73.23</td>\n",
       "      <td>138.450248</td>\n",
       "      <td>3596.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVP</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>991.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLETH_HR</th>\n",
       "      <td>56.5000</td>\n",
       "      <td>67.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3597.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLETH_SPO2</th>\n",
       "      <td>96.9752</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3597.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                q01     q50         q99   n_obs\n",
       "signal                                         \n",
       "ART_MBP      0.0000   73.23  138.450248  3596.5\n",
       "CVP          1.0000    4.00    8.000000   991.0\n",
       "PLETH_HR    56.5000   67.00  100.000000  3597.0\n",
       "PLETH_SPO2  96.9752  100.00  100.000000  3597.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4 — QA summaries\n",
    "print(\"--- Attrition ---\")\n",
    "n_labelled = len(df_labels)\n",
    "n_manifest = len(df_manifest_all)\n",
    "n_usable = len(df_usable)\n",
    "print({\"labelled\": n_labelled, \"has_5_tids\": n_manifest, \"usable_cached\": n_usable})\n",
    "\n",
    "print(\"\\n--- Label distribution (usable only) ---\")\n",
    "usable_caseids = set(pd.to_numeric(df_usable[\"caseid\"], errors=\"coerce\").dropna().astype(int).tolist())\n",
    "df_labels_usable = df_labels[df_labels[\"caseid\"].isin(usable_caseids)].copy()\n",
    "print(df_labels_usable[\"aki\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n--- Missingness report from cache metadata ---\")\n",
    "# Use df_usable obs_* columns if present\n",
    "obs_cols = [c for c in df_usable.columns if c.startswith(\"obs_\")]\n",
    "if obs_cols:\n",
    "    desc = df_usable[obs_cols + ([\"valid_len\"] if \"valid_len\" in df_usable.columns else [])].describe().T\n",
    "    display(desc)\n",
    "else:\n",
    "    print(\"No obs_* columns found in df_usable (maybe all were already cached).\")\n",
    "\n",
    "print(\"\\n--- Sample quantiles by channel (observed values only; from a small sample) ---\")\n",
    "sample_n = min(50, len(df_usable))\n",
    "sample_caseids = df_usable[\"caseid\"].astype(int).head(sample_n).tolist()\n",
    "sig_names = list(CFG.signals)\n",
    "quant_rows = []\n",
    "for cid in tqdm(sample_caseids, desc=\"Quantiles\", unit=\"case\"):\n",
    "    path = CACHE_DIR / f\"case_{cid}.npz\"\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    x = data[\"x\"].astype(np.float32)\n",
    "    n_sig = len(sig_names)\n",
    "    sig = x[:n_sig]\n",
    "    mask = x[n_sig:]\n",
    "    for i, name in enumerate(sig_names):\n",
    "        vals = sig[i][mask[i] > 0.5]\n",
    "        if vals.size == 0:\n",
    "            continue\n",
    "        q = np.quantile(vals, [0.01, 0.5, 0.99]).astype(float)\n",
    "        quant_rows.append({\"caseid\": cid, \"signal\": name, \"q01\": q[0], \"q50\": q[1], \"q99\": q[2], \"n_obs\": int(vals.size)})\n",
    "\n",
    "df_quant = pd.DataFrame(quant_rows)\n",
    "if len(df_quant):\n",
    "    display(df_quant.groupby(\"signal\")[[\"q01\", \"q50\", \"q99\", \"n_obs\"]].median())\n",
    "else:\n",
    "    print(\"No quantiles computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a6a86",
   "metadata": {},
   "source": [
    "## 5) Create StratifiedKFold splits (case-level)\n",
    "This section produces `folds.json` using `caseid` and `aki` (never split by window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b109e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cohort_master: artifacts\\demo_5signals\\cohort_master.csv (2413 cases)\n",
      "Saved folds: artifacts\\demo_5signals\\folds.json\n",
      "   fold  n_train  n_val  train_pos  val_pos\n",
      "0     1     1930    483         81       20\n",
      "1     2     1930    483         81       20\n",
      "2     3     1930    483         80       21\n",
      "3     4     1931    482         81       20\n",
      "4     5     1931    482         81       20\n"
     ]
    }
   ],
   "source": [
    "# Step 5 — StratifiedKFold splits at case level\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "df_master = df_labels_usable.merge(df_manifest_all, on=\"caseid\", how=\"inner\")\n",
    "df_master = df_master.merge(df_usable[[\"caseid\", \"cache_path\"]], on=\"caseid\", how=\"inner\")\n",
    "df_master = df_master.sort_values(\"caseid\").reset_index(drop=True)\n",
    "master_path = ARTIFACTS_DIR / \"cohort_master.csv\"\n",
    "df_master.to_csv(master_path, index=False)\n",
    "print(f\"Saved cohort_master: {master_path} ({len(df_master)} cases)\")\n",
    "\n",
    "caseids = df_master[\"caseid\"].astype(int).to_numpy()\n",
    "y = df_master[\"aki\"].astype(int).to_numpy()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n",
    "folds = []\n",
    "for fold_idx, (tr_idx, va_idx) in enumerate(skf.split(caseids, y), start=1):\n",
    "    folds.append({\n",
    "        \"fold\": fold_idx,\n",
    "        \"train_caseids\": caseids[tr_idx].tolist(),\n",
    "        \"val_caseids\": caseids[va_idx].tolist(),\n",
    "        \"n_train\": int(len(tr_idx)),\n",
    "        \"n_val\": int(len(va_idx)),\n",
    "        \"train_pos\": int(y[tr_idx].sum()),\n",
    "        \"val_pos\": int(y[va_idx].sum()),\n",
    "    })\n",
    "\n",
    "folds_path = ARTIFACTS_DIR / \"folds.json\"\n",
    "_save_json(folds_path, folds)\n",
    "print(f\"Saved folds: {folds_path}\")\n",
    "print(pd.DataFrame(folds)[[\"fold\",\"n_train\",\"n_val\",\"train_pos\",\"val_pos\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7decd99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aki\n",
       "0    2312\n",
       "1     101\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master['aki'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292baea1",
   "metadata": {},
   "source": [
    "## 6) Per-fold normalization stats (mask-aware; train-only)\n",
    "This section computes and saves per-fold scalers without applying them globally (to avoid leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574370a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc4148d622948609b6fc8c408527abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect train vals:   0%|          | 0/1930 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scalers: artifacts\\demo_5signals\\scalers\\scalers_fold1.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96557ddf73b24330bf02b63752cb54a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect train vals:   0%|          | 0/1930 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scalers: artifacts\\demo_5signals\\scalers\\scalers_fold2.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3a333ca756449e986dcc5283e02893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect train vals:   0%|          | 0/1930 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scalers: artifacts\\demo_5signals\\scalers\\scalers_fold3.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2361ac2e5e4719b7c715dde39456a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect train vals:   0%|          | 0/1931 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scalers: artifacts\\demo_5signals\\scalers\\scalers_fold4.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5ef403bdf44d9386b2b80245324d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect train vals:   0%|          | 0/1931 [00:00<?, ?case/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scalers: artifacts\\demo_5signals\\scalers\\scalers_fold5.json\n",
      "Saved fold scalers index: artifacts\\demo_5signals\\fold_scalers_index.json\n",
      "Ready: per-fold scalers computed. Use load_case_tensor_for_fold(caseid, fold_idx) for DL input.\n"
     ]
    }
   ],
   "source": [
    "# Step 6 — Fit per-fold scalers using only observed values (mask==1) from train cases\n",
    "def fit_channel_stats(values: np.ndarray, *, robust: bool) -> Dict[str, float]:\n",
    "    values = values[np.isfinite(values)]\n",
    "    if values.size == 0:\n",
    "        return {\"type\": \"empty\"}\n",
    "    if robust:\n",
    "        med = float(np.median(values))\n",
    "        q25 = float(np.percentile(values, 25))\n",
    "        q75 = float(np.percentile(values, 75))\n",
    "        iqr = float(q75 - q25)\n",
    "        if iqr <= 1e-6:\n",
    "            iqr = 1.0\n",
    "        return {\"type\": \"robust\", \"median\": med, \"iqr\": iqr, \"q25\": q25, \"q75\": q75}\n",
    "    mean = float(values.mean())\n",
    "    std = float(values.std())\n",
    "    if std <= 1e-6:\n",
    "        std = 1.0\n",
    "    return {\"type\": \"z\", \"mean\": mean, \"std\": std}\n",
    "\n",
    "def load_case_x(caseid: int) -> Tuple[np.ndarray, np.ndarray, int]:\n",
    "    path = CACHE_DIR / f\"case_{int(caseid)}.npz\"\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    x = data[\"x\"].astype(np.float32)\n",
    "    valid_len = int(data[\"valid_len\"])\n",
    "    n_sig = len(CFG.signals)\n",
    "    sig = x[:n_sig, :valid_len]\n",
    "    mask = x[n_sig:, :valid_len]\n",
    "    return sig, mask, valid_len\n",
    "\n",
    "def fit_fold_scaler(train_caseids: List[int]) -> Dict[str, Dict[str, float]]:\n",
    "    sig_names = list(CFG.signals)\n",
    "    n_sig = len(sig_names)\n",
    "    # Collect values per channel from train cases (observed only)\n",
    "    collected: List[List[np.ndarray]] = [[] for _ in range(n_sig)]\n",
    "    for cid in tqdm(train_caseids, desc=\"Collect train vals\", unit=\"case\"):\n",
    "        sig, mask, _ = load_case_x(cid)\n",
    "        for i in range(n_sig):\n",
    "            vals = sig[i][mask[i] > 0.5]\n",
    "            if vals.size:\n",
    "                collected[i].append(vals.astype(np.float32))\n",
    "    scalers: Dict[str, Dict[str, float]] = {}\n",
    "    for i, name in enumerate(sig_names):\n",
    "        vals = np.concatenate(collected[i], axis=0) if collected[i] else np.array([], dtype=np.float32)\n",
    "        # Robust for NEPI (and optionally CVP); z-score for others\n",
    "        robust = name in (\"NEPI_RATE\",)\n",
    "        scalers[name] = fit_channel_stats(vals, robust=robust)\n",
    "        # Also store basic quantiles for sanity (not necessarily applied)\n",
    "        if vals.size:\n",
    "            scalers[name].update({\n",
    "                \"q001\": float(np.quantile(vals, 0.001)),\n",
    "                \"q999\": float(np.quantile(vals, 0.999)),\n",
    "                \"n\": int(vals.size),\n",
    "            })\n",
    "        else:\n",
    "            scalers[name].update({\"q001\": None, \"q999\": None, \"n\": 0})\n",
    "    return scalers\n",
    "\n",
    "def apply_scaler_to_signal(name: str, sig: np.ndarray, mask: np.ndarray, scaler: Dict[str, float]) -> np.ndarray:\n",
    "    out = sig.copy()\n",
    "    idx = mask > 0.5\n",
    "    if not idx.any():\n",
    "        return out\n",
    "    if scaler.get(\"type\") == \"robust\":\n",
    "        med = float(scaler[\"median\"])\n",
    "        iqr = float(scaler[\"iqr\"])\n",
    "        out[idx] = (out[idx] - med) / iqr\n",
    "        return out\n",
    "    if scaler.get(\"type\") == \"z\":\n",
    "        mean = float(scaler[\"mean\"])\n",
    "        std = float(scaler[\"std\"])\n",
    "        out[idx] = (out[idx] - mean) / std\n",
    "        return out\n",
    "    return out\n",
    "\n",
    "# Fit and save scalers per fold\n",
    "fold_scalers = []\n",
    "for f in folds:\n",
    "    fold_idx = int(f[\"fold\"])\n",
    "    train_caseids = [int(x) for x in f[\"train_caseids\"]]\n",
    "    scalers = fit_fold_scaler(train_caseids)\n",
    "    out_path = SCALERS_DIR / f\"scalers_fold{fold_idx}.json\"\n",
    "    _save_json(out_path, scalers)\n",
    "    fold_scalers.append({\"fold\": fold_idx, \"path\": str(out_path)})\n",
    "    print(f\"Saved scalers: {out_path}\")\n",
    "\n",
    "fold_scalers_path = ARTIFACTS_DIR / \"fold_scalers_index.json\"\n",
    "_save_json(fold_scalers_path, fold_scalers)\n",
    "print(f\"Saved fold scalers index: {fold_scalers_path}\")\n",
    "\n",
    "# Optional helper: load normalized tensor for a fold (for downstream DL model)\n",
    "def load_case_tensor_for_fold(caseid: int, fold_idx: int) -> Tuple[np.ndarray, int]:\n",
    "    scalers_path = SCALERS_DIR / f\"scalers_fold{int(fold_idx)}.json\"\n",
    "    scalers = json.loads(scalers_path.read_text(encoding=\"utf-8\"))\n",
    "    sig, mask, valid_len = load_case_x(caseid)\n",
    "    sig_names = list(CFG.signals)\n",
    "    for i, name in enumerate(sig_names):\n",
    "        sig[i] = apply_scaler_to_signal(name, sig[i], mask[i], scalers[name]).astype(np.float32)\n",
    "    x = np.concatenate([sig, mask], axis=0).astype(np.float32)\n",
    "    return x, valid_len\n",
    "\n",
    "print(\"Ready: per-fold scalers computed. Use load_case_tensor_for_fold(caseid, fold_idx) for DL input.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
